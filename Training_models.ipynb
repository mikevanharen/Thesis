{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Training_models.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1N4hH30QuOgh",
        "fH0YD6E_Vt7X",
        "0zXWrEYsFMX7",
        "ZMr-Kmq2XHjO",
        "Ct5AP18JN92f",
        "IKlnrdIJXHjU",
        "S-4OIxj0XHjV",
        "tf642aLmmXBg",
        "f_O49EvZuz0w",
        "wdp8cnlLnamE",
        "4bd9AZ9wZ2YX",
        "cL7NPk-MaSqv"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6-final"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvpVgEH8BO9M"
      },
      "source": [
        "# Pipeline:\n",
        "After importing the necessary functions, we load the data from the 'movie_labels.csv' file. For every genre, we make the list of movie_labels where we have a label for every movie. Additionally for every genre, we make the lists of all movies, the movies of that genre, and the movies not belonging to that genre. Then, for every genre, we use these lists and the amount of movies in that genre to create the training, validation, and test splits of the data.\n",
        "\n",
        "In *Creating my own way to use and load the data* we use these splits and get the labels of every movie for every genre. Then, these are used to create the dataloaders. Now we have everything we need to start training the models. We create the train_model and test_model function. Also, we load a pretrained (on ImageNet) resnet18 model as the starting model. Then, using the train_model function, we train a classifier for every genre.\n",
        "\n",
        "In *New training code, for using the profiles in the validation*, we start working on a different classifier for every genre. We want to take a look at what happens if we use profile level validation instead of single image level validation (that's what we used for the first kind of models). To do this, we have to load the profiles from 'user_profiles.csv'. Then, we can get the labels of all the profiles, based on the amount of movies in the profile of the genre we're looking at. Now, we again make the lists of positive elements and negative elements for every genre. This was single movies first, but here this contains the profiles. Then, we create the training, validation, and test split. For which we get the labels and create the dataloaders.\n",
        "\n",
        "With the newly created new_train_model function, we train these other classifiers. We use the same training set as before, but we use the profiles for validation. After training these models as well, we use the test_model function. With this function we can see how well all models perform on the single-image level test split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Fs_YWt6XHjG"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2X12e-yXHjN"
      },
      "source": [
        "# License: BSD\n",
        "# Author: Sasank Chilamkurthy\n",
        "from __future__ import print_function, division\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "\n",
        "import csv\n",
        "import random\n",
        "import math\n",
        "\n",
        "from PIL import Image, ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "plt.ion()   # interactive mode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOAX9v1quGLI"
      },
      "source": [
        "# Connect to drive and unzip the images\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udIXL8vFkQp1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDC73SpwlaAn"
      },
      "source": [
        "!unzip 'drive/MyDrive/movies_data_split.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1N4hH30QuOgh"
      },
      "source": [
        "# Load the data and create the train/val/test splits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AY4M34LypfQy"
      },
      "source": [
        "def updateDict(movies, genre, movie_id):\n",
        "  movie_list = movies.get(genre, [])\n",
        "  movie_list.append(movie_id)\n",
        "\n",
        "  return movie_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u36VKSn17E5w"
      },
      "source": [
        "genre_dict = {'Action':0, 'Adventure':1, 'Animation_Children\\'s':2, 'Comedy':3, 'Crime':4, \n",
        "              'Documentary':5, 'Drama':6, 'Fantasy_Sci-Fi':7, 'Film-Noir':8, 'Horror_Thriller':9, \n",
        "              'Musical':10, 'Mystery':11, 'Romance':12, 'War':13, 'Western':14}\n",
        "row_names = ['movie_id', 'genre_array']\n",
        "count = {}\n",
        "\n",
        "# Separate list for all movies, movies that have the genre, and those that don't. Finally, the movies used for testing\n",
        "all_movies = {}\n",
        "movies = {}\n",
        "non_movies = {}\n",
        "test_movies = {}\n",
        "\n",
        "# Dictionary for every movie, for every genre. To be used later, in loading the user's movies.\n",
        "movie_labels = {} # movie_labels[movie][genre]\n",
        "\n",
        "with open('drive/MyDrive/movie_labels.csv', 'r', encoding = \"ISO-8859-1\") as f:\n",
        "    reader = csv.DictReader(f, fieldnames=row_names, delimiter=',')\n",
        "    for row in reader:\n",
        "      movie_id = int(row['movie_id'])\n",
        "        \n",
        "      my_table = row['genre_array'].maketrans('','','[ ]')\n",
        "      genre_string_array = list(row['genre_array'].translate(my_table).split(','))  \n",
        "      genre_array = [int(s) for s in genre_string_array]\n",
        "\n",
        "      # Make movie_labels list\n",
        "      movie_labels[movie_id] = {}\n",
        "\n",
        "      for genre in genre_dict.keys():\n",
        "        # if movie_id < 3564:\n",
        "        if genre_array[genre_dict[genre]] == 1:\n",
        "          c = count.get(genre, 0)\n",
        "          count[genre] = c + 1\n",
        "          \n",
        "          all_movies[genre] = updateDict(all_movies, genre, movie_id)\n",
        "          movies[genre] = updateDict(movies, genre, movie_id)\n",
        "\n",
        "          # Make movie_labels list\n",
        "          movie_labels[movie_id][genre] = 1\n",
        "\n",
        "        else:\n",
        "          all_movies[genre] = updateDict(all_movies, genre, movie_id)\n",
        "          non_movies[genre] = updateDict(non_movies, genre, movie_id)\n",
        "\n",
        "          # Make movie_labels list\n",
        "          movie_labels[movie_id][genre] = 0\n",
        "\n",
        "print(\"Done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "171zCPi904Go"
      },
      "source": [
        "# Visualise what's inside movie_labels and its construction\n",
        "print(movie_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxQDlx_zAn41"
      },
      "source": [
        "train_N = {}\n",
        "val_N   = {}\n",
        "test_N  = {}\n",
        "\n",
        "train_split = {}\n",
        "val_split   = {}\n",
        "test_split  = {}\n",
        "\n",
        "for genre in genre_dict.keys():\n",
        "  train_N[genre] = math.ceil(count[genre]*0.8)\n",
        "  val_N[genre] = math.floor(count[genre]*0.1)\n",
        "  test_N[genre] = math.floor(count[genre]*0.1)\n",
        "\n",
        "  train_split[genre] = movies[genre][:train_N[genre]] + non_movies[genre][:train_N[genre]]\n",
        "  val_split[genre]   = movies[genre][train_N[genre]:train_N[genre]+val_N[genre]] + non_movies[genre][train_N[genre]:train_N[genre]+val_N[genre]]\n",
        "  test_split[genre]  = movies[genre][train_N[genre]+val_N[genre]:] + non_movies[genre][train_N[genre]+val_N[genre]:train_N[genre]+val_N[genre]+test_N[genre]]\n",
        "\n",
        "print(train_N)\n",
        "print(val_N)\n",
        "print(test_N)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fH0YD6E_Vt7X"
      },
      "source": [
        "# Creating my own way to use and load the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zXWrEYsFMX7"
      },
      "source": [
        "## Creating the dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMfaTF_YVsWB"
      },
      "source": [
        "partition = {}\n",
        "train_labels = {}\n",
        "val_labels   = {}\n",
        "test_labels  = {}\n",
        "\n",
        "# Loop over the genres\n",
        "for genre in genre_dict.keys():\n",
        "  partition[genre] = {'train':train_split[genre], 'val':val_split[genre], 'test':test_split[genre]}\n",
        "  train_labels[genre] = {}\n",
        "  val_labels[genre]   = {}\n",
        "  test_labels[genre]  = {}\n",
        "\n",
        "  # Loop over the movies in train_split. For every movie add the label.\n",
        "  for movie in train_split[genre]:\n",
        "    if movie in movies[genre]:\n",
        "      train_labels[genre][movie] = torch.LongTensor([1])\n",
        "    else:\n",
        "      train_labels[genre][movie] = torch.LongTensor([0])\n",
        "\n",
        "  # Loop over the movies in val_split. For every movie add the label.\n",
        "  for movie in val_split[genre]:\n",
        "    if movie in movies[genre]:\n",
        "      val_labels[genre][movie] = torch.LongTensor([1])\n",
        "    else:\n",
        "      val_labels[genre][movie] = torch.LongTensor([0])\n",
        "\n",
        "\n",
        "  # Loop over the movies in test_split. For every movie add the label.\n",
        "  for movie in test_split[genre]:\n",
        "    if movie in movies[genre]:\n",
        "      test_labels[genre][movie] = torch.LongTensor([1])\n",
        "    else:\n",
        "      test_labels[genre][movie] = torch.LongTensor([0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "le3NviOYZoGM"
      },
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "  'Characterizes a dataset for PyTorch'\n",
        "  def __init__(self, list_IDs, labels, YOUR_TRANSFORM):\n",
        "    'Initialization'\n",
        "    self.labels = labels\n",
        "    self.list_IDs = list_IDs\n",
        "    self.YOUR_TRANSFORM = YOUR_TRANSFORM\n",
        "\n",
        "  def __len__(self):\n",
        "    'Denotes the total number of samples'\n",
        "    return len(self.list_IDs)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      'Generates one sample of data'\n",
        "      # Select sample\n",
        "      ID = self.list_IDs[index]\n",
        "\n",
        "      # Load data and get label\n",
        "      X = self.YOUR_TRANSFORM(Image.open('movies_data_split/' + str(ID) + '.jpg').convert('RGB')) # here X should be a torch.Tensor\n",
        "      y = self.labels[ID][0] # it should also be a torch tensor torch.LongTensor(self.labels[ID] )\n",
        "\n",
        "      return X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8F613u7XHjN"
      },
      "source": [
        "# Data augmentation and normalization for training\n",
        "# Just normalization for validation\n",
        "\n",
        "# ZL: maybe start with this setup. If it does not work, we can discuss how to improve.\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),#ZL: think about your own case\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "dataloaders = {}\n",
        "dataset_sizes = {}\n",
        "\n",
        "for genre in genre_dict.keys():\n",
        "  train_set = Dataset(partition[genre]['train'], train_labels[genre], data_transforms['train'])\n",
        "  train_generator = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "  val_set = Dataset(partition[genre]['val'], val_labels[genre], data_transforms['val'])\n",
        "  val_generator = torch.utils.data.DataLoader(val_set, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "  test_set = Dataset(partition[genre]['test'], test_labels[genre], data_transforms['test'])\n",
        "  test_generator = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "  dataset_sizes[genre] = {x: len(partition[genre][x]) for x in ['train', 'val', 'test']}\n",
        "  \n",
        "  # class_names\n",
        "  class_names=[0,1]\n",
        "\n",
        "  dataloaders[genre] = {'train':train_generator,\n",
        "                        'val':val_generator,\n",
        "                        'test':test_generator}\n",
        "               \n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMr-Kmq2XHjO"
      },
      "source": [
        "## Train_model function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTl_1AiKXHjP"
      },
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, genre, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[genre][phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[genre][phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[genre][phase]\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ct5AP18JN92f"
      },
      "source": [
        "## Test_model function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmbKp0it48Hc"
      },
      "source": [
        "def test_model(model, genre):\n",
        "  model.eval()   # Set model to evaluate mode\n",
        "  device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "  if genre in ['Mystery', 'Romance', 'War']:\n",
        "    device = torch.device('cpu')\n",
        "  \n",
        "  running_corrects = 0\n",
        "\n",
        "  # Iterate over data.\n",
        "  for inputs, labels in dataloaders[genre]['test']:\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    outputs = model(inputs)\n",
        "    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "    running_corrects += torch.sum(preds == labels.data)\n",
        "  \n",
        "  acc = running_corrects.double() / dataset_sizes[genre]['test']\n",
        "  print('test Acc: {:.4f}'.format(acc))\n",
        "\n",
        "  return preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKlnrdIJXHjU"
      },
      "source": [
        "## Load a pretrained model and reset final fully connected layer.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCq7-XX0XHjU"
      },
      "source": [
        "model_ft = models.resnet18(pretrained=True) \n",
        "num_ftrs = model_ft.fc.in_features\n",
        "# Here the size of each output sample is set to 2.\n",
        "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
        "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-4OIxj0XHjV"
      },
      "source": [
        "## Train the models\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPeFZxkb5r0n"
      },
      "source": [
        "trained_models = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plxOZsLnNgZa"
      },
      "source": [
        "for genre in genre_dict.keys():\n",
        "  print(genre)\n",
        "  trained_models[genre] = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, \n",
        "                          genre, num_epochs=25)\n",
        "  torch.save(trained_models[genre], '{}_model'.format(genre))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf642aLmmXBg"
      },
      "source": [
        "# New training code, for using the profiles in the validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_O49EvZuz0w"
      },
      "source": [
        "## Load the profiles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8bVWGQ14vxR"
      },
      "source": [
        "row_names = ['user', 'movies']\n",
        "user_labels = {}\n",
        "\n",
        "with open('drive/MyDrive/user_profiles.csv', 'r', encoding = \"ISO-8859-1\") as f:\n",
        "    reader = csv.DictReader(f, fieldnames=row_names, delimiter=',')\n",
        "    for row in reader:\n",
        "      user = row['user']\n",
        "\n",
        "      my_table = row['movies'].maketrans('', '', '[ ]')\n",
        "      movie_string_array = list(row['movies'].translate(my_table).split(','))\n",
        "      movies = [int(m) for m in movie_string_array]\n",
        "\n",
        "      user_labels[user] = {}\n",
        "\n",
        "      for genre in genre_dict.keys():\n",
        "        user_labels[user][genre] = {}\n",
        "\n",
        "        for movie in movies:\n",
        "          if movie != 1107:\n",
        "            user_labels[user][genre][movie] = movie_labels[movie][genre]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdp8cnlLnamE"
      },
      "source": [
        "## The true labels of the profiles\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tKSfiDDtePI"
      },
      "source": [
        "true_labels = {}\n",
        "\n",
        "for user in user_labels.keys():\n",
        "  true_labels[user] = {}\n",
        "  for genre in genre_dict.keys():\n",
        "    movies = list(user_labels[user][genre].keys())\n",
        "    count = 0\n",
        "    for movie in movies:\n",
        "      count += user_labels[user][genre][movie]\n",
        "\n",
        "    fraction = count/len(movies)\n",
        "    if fraction >= 0.25:\n",
        "      true_labels[user][genre] = 1\n",
        "    else:\n",
        "      true_labels[user][genre] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bd9AZ9wZ2YX"
      },
      "source": [
        "## The list of sampled users"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCiNTiTA0AKd"
      },
      "source": [
        "# Get the count to make train, val, test splits.\n",
        "count = {}\n",
        "users = {}\n",
        "non_users = {}\n",
        "\n",
        "for genre in genre_dict.keys():\n",
        "  count[genre] = 0\n",
        "  users[genre] = []\n",
        "  non_users[genre] = []\n",
        "\n",
        "  for u in true_labels.keys():\n",
        "    if true_labels[u][genre] == 1:\n",
        "      users[genre].append(u)\n",
        "      count[genre] +=1\n",
        "    else:\n",
        "      non_users[genre].append(u)\n",
        "  \n",
        "  # The count should be the amount of the smallest side (1's or 0's)\n",
        "  # to be able to get an equal amount of 1's and 0's in every split.\n",
        "  if count[genre] > len(true_labels)/2: # Comedy, Drama\n",
        "    count[genre] = len(true_labels) - count[genre]\n",
        "  print(genre, count[genre])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cL7NPk-MaSqv"
      },
      "source": [
        "## The train/val/test splits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sz1uAhylz6JK"
      },
      "source": [
        "train_N = {}\n",
        "val_N   = {}\n",
        "test_N  = {}\n",
        "\n",
        "train_split = {}\n",
        "val_split   = {}\n",
        "test_split  = {}\n",
        "\n",
        "for genre in genre_dict.keys():\n",
        "  val_N[genre] = math.floor(count[genre]*0.1)\n",
        "  test_N[genre] = math.floor(count[genre]*0.1)\n",
        "  train_N[genre] = count[genre] - val_N[genre] - test_N[genre]\n",
        "\n",
        "  if val_N[genre] == 0: # There are not a lot of profiles with documentary as 1.\n",
        "    val_N[genre] = 1\n",
        "    test_N[genre] = 1\n",
        "    train_N[genre] = count[genre] - 2\n",
        "\n",
        "  train_split[genre] = users[genre][:train_N[genre]] + non_users[genre][:train_N[genre]]\n",
        "  val_split[genre]   = users[genre][train_N[genre]:train_N[genre]+val_N[genre]] + non_users[genre][train_N[genre]:train_N[genre]+val_N[genre]]\n",
        "  test_split[genre]  = users[genre][train_N[genre]+val_N[genre]:] + non_users[genre][train_N[genre]+val_N[genre]:train_N[genre]+val_N[genre]+test_N[genre]]\n",
        "\n",
        "print(train_N)\n",
        "print(val_N)\n",
        "print(test_N)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_bieOJfaquY"
      },
      "source": [
        "## Labels for dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sunmyYlQ4N4q"
      },
      "source": [
        "labels = {}\n",
        "movies = {}\n",
        "movie_index = {}\n",
        "profile_labels = {'train':{},\n",
        "                  'val':{},\n",
        "                  'test':{}}\n",
        "\n",
        "for genre in genre_dict.keys():\n",
        "  labels[genre] = {}\n",
        "  movies[genre] = {}\n",
        "  movie_index[genre] = {}\n",
        "  profile_labels['train'][genre] = {}\n",
        "  profile_labels['val'][genre] = {}\n",
        "  profile_labels['test'][genre] = {}\n",
        "\n",
        "  # Make the movie list and index list for val split.\n",
        "  movies[genre]['train'] = []\n",
        "  for user in train_split[genre]:   \n",
        "    count = 0\n",
        "    for movie in list(user_labels[user][genre].keys()):\n",
        "      movies[genre]['train'].append(movie)\n",
        "      count += movie_labels[movie][genre]\n",
        "\n",
        "    fraction = count / len(user_labels[user][genre])\n",
        "    if fraction >= 0.25:\n",
        "      profile_labels['train'][genre][user] = 1\n",
        "    else:\n",
        "      profile_labels['train'][genre][user] = 0\n",
        "\n",
        "\n",
        "  # Make the movie list and index list for val split.\n",
        "  movies[genre]['val'] = []\n",
        "  for user in val_split[genre]:   \n",
        "    count = 0\n",
        "    for movie in list(user_labels[user][genre].keys()):\n",
        "      movies[genre]['val'].append(movie)\n",
        "      count += movie_labels[movie][genre]\n",
        "\n",
        "    fraction = count / len(user_labels[user][genre])\n",
        "    if fraction >= 0.25:\n",
        "      profile_labels['val'][genre][user] = 1\n",
        "    else:\n",
        "      profile_labels['val'][genre][user] = 0\n",
        "\n",
        "  # Make the movie list and index list for test split.\n",
        "  movies[genre]['test'] = []\n",
        "  for user in test_split[genre]:\n",
        "    count = 0\n",
        "    for movie in list(user_labels[user][genre].keys()):\n",
        "      movies[genre]['test'].append(movie)\n",
        "      count += movie_labels[movie][genre]\n",
        "    \n",
        "    fraction = count / len(user_labels[user][genre])\n",
        "    if fraction >= 0.25:\n",
        "      profile_labels['test'][genre][user] = 1\n",
        "    else:\n",
        "      profile_labels['test'][genre][user] = 0\n",
        "    \n",
        "  # Remove duplicates\n",
        "  for phase in ['train', 'val', 'test']:\n",
        "    movies[genre][phase] = list(set(movies[genre][phase]))\n",
        "    labels[genre][phase] = {}\n",
        "    movie_index[genre][phase] = {}\n",
        "\n",
        "    index = 0\n",
        "    for movie in movies[genre][phase]:\n",
        "      movie_index[genre][phase][movie] = index\n",
        "      labels[genre][phase][movie] = torch.LongTensor([movie_labels[movie][genre]])\n",
        "      index += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpUg3pxeahsp"
      },
      "source": [
        "## Make dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYNxEF_L3r2n"
      },
      "source": [
        "profile_dataset_sizes = {}\n",
        "profile_dataloaders = {}\n",
        "\n",
        "for phase in ['train', 'val', 'test']:\n",
        "  profile_dataset_sizes[phase] = {}\n",
        "  profile_dataloaders[phase]   = {}\n",
        "\n",
        "  for genre in genre_dict.keys():\n",
        "    dataset = Dataset(movies[genre][phase], labels[genre][phase], data_transforms[phase])\n",
        "    profile_generator = torch.utils.data.DataLoader(dataset, batch_size=128, shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "    profile_dataloaders[phase][genre] = profile_generator\n",
        "\n",
        "  for user in user_labels.keys():\n",
        "    profile_dataset_sizes[phase][user] = len(user_labels[user][genre])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DL6SxBJauqg"
      },
      "source": [
        "# New_train_model function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvzbXazjkMgd"
      },
      "source": [
        "def new_train_model(model, criterion, optimizer, scheduler, genre, profiles, true_labels, movie_index, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            if phase == 'train':\n",
        "              # Iterate over data.\n",
        "              for inputs, labels in dataloaders[genre][phase]:\n",
        "                  inputs = inputs.to(device)\n",
        "                  labels = labels.to(device)\n",
        "\n",
        "                  # zero the parameter gradients\n",
        "                  optimizer.zero_grad()\n",
        "\n",
        "                  # forward\n",
        "                  # track history if only in train\n",
        "                  with torch.set_grad_enabled(phase == 'train'):\n",
        "                      outputs = model(inputs)\n",
        "                      _, preds = torch.max(outputs, 1)\n",
        "                      loss = criterion(outputs, labels)\n",
        "\n",
        "                      # backward + optimize only if in training phase\n",
        "                      loss.backward()\n",
        "                      optimizer.step()\n",
        "\n",
        "                  # statistics\n",
        "                  running_loss += loss.item() * inputs.size(0)\n",
        "                  running_corrects += torch.sum(preds == labels.data)\n",
        "              \n",
        "              scheduler.step()\n",
        "\n",
        "              epoch_loss = running_loss / dataset_sizes[genre][phase]\n",
        "              epoch_acc = running_corrects.double() / dataset_sizes[genre][phase]\n",
        "\n",
        "            else: #phase == 'val'\n",
        "              predictions = []\n",
        "\n",
        "              # Iterate over data.              \n",
        "              for inputs, labels in profile_dataloaders['val'][genre]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                \n",
        "                predictions += [int(p) for p in list(preds)]\n",
        "                loss = criterion(outputs, labels)\n",
        "                \n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)          # Not sure if this running_loss should be here, same with running_corrects\n",
        "\n",
        "              for user in profiles:\n",
        "                count = 0\n",
        "                movies = user_labels[user][genre]\n",
        "\n",
        "                # count is the amount of movies positively classified for this genre\n",
        "                for movie in movies:\n",
        "                  index = movie_index[int(movie)]\n",
        "                  count += predictions[index]\n",
        "\n",
        "                fraction = count/profile_dataset_sizes[phase][user]\n",
        "                \n",
        "                if fraction >= 0.25:\n",
        "                  profile_pred = 1\n",
        "                else:\n",
        "                  profile_pred = 0\n",
        "                \n",
        "                # print(user, fraction, profile_pred, true_labels[genre][user])\n",
        "                # print(count, profile_dataset_sizes[phase][user])\n",
        "                # print()\n",
        "\n",
        "                if profile_pred == true_labels[genre][user]:\n",
        "                  running_corrects += 1\n",
        "                else:\n",
        "                  running_corrects += 0\n",
        "\n",
        "              # epoch_loss = running_loss / len(profiles)\n",
        "              epoch_loss = running_loss / dataset_sizes[genre][phase]\n",
        "              epoch_acc = running_corrects / len(profiles) # running_corrects.double() # Because it was a tensor.\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JDrgOxDw8oY"
      },
      "source": [
        "## Load a pretrained model and reset final fully connected layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJhYLNnTR5yU"
      },
      "source": [
        "new_model_ft = models.resnet18(pretrained=True) \n",
        "num_ftrs = new_model_ft.fc.in_features\n",
        "# Here the size of each output sample is set to 2.\n",
        "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
        "new_model_ft.fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "new_model_ft = new_model_ft.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(new_model_ft.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPFqCkj3vlVn"
      },
      "source": [
        "## Train the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Br7FTmF2Om1C"
      },
      "source": [
        "new_trained_models = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bg7OqpOstRlK"
      },
      "source": [
        "for genre in genre_dict.keys():\n",
        "  print(genre)\n",
        "  new_trained_models[genre] = new_train_model(new_model_ft, criterion, optimizer_ft, exp_lr_scheduler, \n",
        "                          genre, val_split[genre], profile_labels['val'], movie_index[genre]['val'], num_epochs=10)\n",
        "  torch.save(new_trained_models[genre], 'new_{}_model'.format(genre))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp91fL1c5pw4"
      },
      "source": [
        "# Test the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkuTiV176Bw9"
      },
      "source": [
        "# Run this block if you save the models already\n",
        "trained_models = {}\n",
        "new_trained_models = {}\n",
        "for genre in genre_dict.keys():\n",
        "  trained_models[genre] = torch.load('drive/MyDrive/models/{}_model'.format(genre), map_location=torch.device('cpu'))\n",
        "  new_trained_models[genre] = torch.load('drive/MyDrive/models/new_{}_model'.format(genre), map_location=torch.device('cpu'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNMFGy2x6R4K"
      },
      "source": [
        "for genre in genre_dict.keys():\n",
        "  print(genre)\n",
        "  test_model(trained_models[genre], genre)\n",
        "  test_model(new_trained_models[genre], genre)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}